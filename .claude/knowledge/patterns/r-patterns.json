{
  "language": "r",
  "patterns": {
    "tidy_pipeline": {
      "description": "Use the pipe operator %>% to create readable data transformation pipelines.",
      "whenToUse": "For sequential data transformations and maintaining code readability.",
      "example": "library(tidyverse)\n\n# Basic pipeline\nresult <- mtcars %>%\n  filter(mpg > 20) %>%\n  select(mpg, cyl, wt, hp) %>%\n  mutate(power_to_weight = hp / wt) %>%\n  arrange(desc(power_to_weight)) %>%\n  group_by(cyl) %>%\n  summarise(\n    avg_mpg = mean(mpg),\n    avg_power_ratio = mean(power_to_weight),\n    n = n(),\n    .groups = 'drop'\n  )\n\n# Complex pipeline with multiple operations\nsales_analysis <- raw_data %>%\n  # Clean data\n  filter(!is.na(sales), sales > 0) %>%\n  mutate(\n    date = as.Date(date_string, \"%Y-%m-%d\"),\n    month = lubridate::month(date, label = TRUE),\n    quarter = lubridate::quarter(date)\n  ) %>%\n  # Join with product info\n  left_join(product_catalog, by = \"product_id\") %>%\n  # Calculate metrics\n  group_by(category, month) %>%\n  summarise(\n    total_sales = sum(sales),\n    avg_price = mean(price),\n    units_sold = n(),\n    .groups = 'drop'\n  ) %>%\n  # Add running totals\n  group_by(category) %>%\n  arrange(month) %>%\n  mutate(\n    cumulative_sales = cumsum(total_sales),\n    month_over_month = total_sales / lag(total_sales) - 1\n  ) %>%\n  ungroup()\n\n# Pipeline with custom functions\nprocess_text <- function(text) {\n  text %>%\n    str_to_lower() %>%\n    str_trim() %>%\n    str_replace_all(\"[^a-z0-9\\\\s]\", \"\") %>%\n    str_squish()\n}\n\ntext_data %>%\n  mutate(clean_text = map_chr(text, process_text)) %>%\n  unnest_tokens(word, clean_text) %>%\n  anti_join(stop_words, by = \"word\") %>%\n  count(word, sort = TRUE) %>%\n  slice_head(n = 20) %>%\n  ggplot(aes(x = reorder(word, n), y = n)) +\n  geom_col() +\n  coord_flip() +\n  labs(x = \"Word\", y = \"Frequency\")"
    },
    "functional_programming": {
      "description": "Use purrr functions for functional programming with map, reduce, and compose.",
      "whenToUse": "For applying functions to lists, avoiding loops, and creating reusable transformations.",
      "example": "library(purrr)\n\n# Map functions\n# map() returns a list, map_*() returns typed vectors\ndata_files <- list.files(\"data/\", pattern = \"\\\\.csv$\", full.names = TRUE)\n\n# Read all CSV files into a list\nall_data <- map(data_files, read_csv)\n\n# Read and combine with file info\ndata_with_source <- map2_dfr(\n  data_files,\n  basename(data_files),\n  ~ read_csv(.x) %>% mutate(source_file = .y)\n)\n\n# Map over multiple inputs\nmodels <- list(\n  linear = lm(mpg ~ wt, data = mtcars),\n  quadratic = lm(mpg ~ wt + I(wt^2), data = mtcars),\n  interaction = lm(mpg ~ wt * hp, data = mtcars)\n)\n\n# Extract R-squared values\nr_squared <- map_dbl(models, ~ summary(.x)$r.squared)\n\n# Safely handle errors\nsafe_log <- safely(log)\nresults <- map(c(10, -5, 0, 20), safe_log)\n\n# Extract successful results\nvalid_results <- results %>%\n  keep(~ is.null(.x$error)) %>%\n  map_dbl(\"result\")\n\n# Reduce functions\n# Combine multiple data frames\ncombined_df <- reduce(all_data, bind_rows)\n\n# Custom reduction\ncustom_merge <- function(df1, df2) {\n  full_join(df1, df2, by = \"id\") %>%\n    mutate(across(where(is.numeric), ~ coalesce(.x, 0)))\n}\n\nmerged_data <- reduce(data_list, custom_merge)\n\n# Function composition\nprocess_pipeline <- compose(\n  ~ filter(.x, value > 0),\n  ~ mutate(.x, log_value = log(value)),\n  ~ arrange(.x, desc(log_value)),\n  ~ slice_head(.x, n = 10)\n)\n\ntop_values <- process_pipeline(raw_data)\n\n# Partial application\nadd_percentage <- partial(paste0, ... = \"%\")\npercentages <- map_chr(c(10, 25, 50), add_percentage)\n\n# Walk for side effects\nplots <- mtcars %>%\n  split(.$cyl) %>%\n  map(~ ggplot(.x, aes(wt, mpg)) + \n         geom_point() + \n         geom_smooth(method = \"lm\"))\n\nwalk2(\n  plots,\n  names(plots),\n  ~ ggsave(paste0(\"plot_cyl_\", .y, \".png\"), .x)\n)"
    },
    "s3_classes": {
      "description": "Create S3 classes for object-oriented programming with generic functions.",
      "whenToUse": "For creating custom data types with associated methods.",
      "example": "# Define a constructor\ntime_series <- function(data, dates, frequency = \"daily\") {\n  # Validate inputs\n  stopifnot(\n    is.numeric(data),\n    inherits(dates, \"Date\") || inherits(dates, \"POSIXct\"),\n    length(data) == length(dates),\n    frequency %in% c(\"daily\", \"weekly\", \"monthly\", \"quarterly\", \"yearly\")\n  )\n  \n  # Create object\n  structure(\n    list(\n      data = data,\n      dates = dates,\n      frequency = frequency,\n      start_date = min(dates),\n      end_date = max(dates),\n      n_obs = length(data)\n    ),\n    class = c(\"time_series\", \"list\")\n  )\n}\n\n# Generic methods\nprint.time_series <- function(x, ...) {\n  cat(\"Time Series Object\\n\")\n  cat(\"Frequency:\", x$frequency, \"\\n\")\n  cat(\"Date range:\", as.character(x$start_date), \"to\", as.character(x$end_date), \"\\n\")\n  cat(\"Observations:\", x$n_obs, \"\\n\")\n  cat(\"\\nFirst 6 values:\\n\")\n  print(head(data.frame(date = x$dates, value = x$data)))\n  invisible(x)\n}\n\nsummary.time_series <- function(object, ...) {\n  list(\n    frequency = object$frequency,\n    date_range = c(start = object$start_date, end = object$end_date),\n    statistics = summary(object$data),\n    missing = sum(is.na(object$data)),\n    trend = if(length(object$data) > 2) {\n      coef(lm(object$data ~ seq_along(object$data)))[2]\n    } else NA\n  )\n}\n\nplot.time_series <- function(x, type = \"line\", ...) {\n  df <- data.frame(date = x$dates, value = x$data)\n  \n  p <- ggplot(df, aes(x = date, y = value))\n  \n  if (type == \"line\") {\n    p <- p + geom_line()\n  } else if (type == \"point\") {\n    p <- p + geom_point()\n  } else if (type == \"both\") {\n    p <- p + geom_line() + geom_point()\n  }\n  \n  p + \n    labs(\n      title = paste(x$frequency, \"Time Series\"),\n      x = \"Date\",\n      y = \"Value\"\n    ) +\n    theme_minimal()\n}\n\n# Additional methods\ntransform.time_series <- function(`_data`, ...) {\n  transforms <- list(...)\n  \n  for (name in names(transforms)) {\n    if (name == \"log\") {\n      `_data`$data <- log(`_data`$data)\n    } else if (name == \"diff\") {\n      `_data`$data <- c(NA, diff(`_data`$data))\n    } else if (name == \"scale\") {\n      `_data`$data <- scale(`_data`$data)[, 1]\n    }\n  }\n  \n  `_data`\n}\n\n# Subset method\n`[.time_series` <- function(x, i) {\n  time_series(\n    data = x$data[i],\n    dates = x$dates[i],\n    frequency = x$frequency\n  )\n}\n\n# Usage\ndates <- seq.Date(from = as.Date(\"2023-01-01\"), by = \"day\", length.out = 100)\nvalues <- cumsum(rnorm(100)) + 100\n\nts <- time_series(values, dates, \"daily\")\nprint(ts)\nsummary(ts)\nplot(ts)\n\n# Transform\nts_log <- transform(ts, log = TRUE, scale = TRUE)\n\n# Subset\nts_subset <- ts[1:30]"
    },
    "custom_operators": {
      "description": "Define custom infix operators for domain-specific operations.",
      "whenToUse": "For creating expressive domain-specific languages or common operations.",
      "example": "# Define custom operators\n# Pattern: %operator_name%\n\n# Set operations\n`%not_in%` <- function(x, table) {\n  !(x %in% table)\n}\n\n# Usage\nfiltered <- data %>%\n  filter(category %not_in% c(\"excluded\", \"invalid\"))\n\n# Null coalescing operator\n`%||%` <- function(x, y) {\n  if (is.null(x) || length(x) == 0 || all(is.na(x))) y else x\n}\n\n# Usage\nconfig_value <- user_setting %||% default_setting %||% \"fallback\"\n\n# String concatenation\n`%+%` <- function(x, y) {\n  paste0(x, y)\n}\n\n# Usage\ngreeting <- \"Hello, \" %+% username %+% \"!\"\n\n# Pipeline with error handling\n`%?>%` <- function(x, f) {\n  tryCatch(\n    f(x),\n    error = function(e) {\n      warning(\"Pipeline error: \", e$message)\n      NULL\n    }\n  )\n}\n\n# Usage\nresult <- risky_data %?>%\n  filter(value > 0) %?>%\n  mutate(log_value = log(value)) %?>%\n  summarise(mean_log = mean(log_value))\n\n# Interval checking\n`%between%` <- function(x, range) {\n  x >= range[1] & x <= range[2]\n}\n\n# Usage\nvalid_ages <- ages[ages %between% c(18, 65)]\n\n# Function composition\n`%>>%` <- function(f, g) {\n  function(...) g(f(...))\n}\n\n# Usage\nprocess <- tolower %>>% \n           trimws %>>% \n           {\\(x) gsub(\"[^a-z]\", \"\", x)}\n\ncleaned <- process(\"  Hello, World!  \")\n\n# Matching operator with multiple patterns\n`%~%` <- function(x, patterns) {\n  any(sapply(patterns, function(p) grepl(p, x)))\n}\n\n# Usage\nemail_domains <- emails[emails %~% c(\"@gmail\", \"@yahoo\", \"@hotmail\")]\n\n# Safe division\n`%/%` <- function(x, y) {\n  ifelse(y == 0, NA, x / y)\n}\n\n# Pipe-friendly assignment\n`%<>%` <- function(x, f) {\n  assign(deparse(substitute(x)), f(x), envir = parent.frame())\n  invisible(x)\n}\n\n# Usage\ndata %<>% filter(complete.cases(.))"
    },
    "functional_factories": {
      "description": "Create functions that generate other functions with captured state.",
      "whenToUse": "For creating parameterized functions and avoiding code duplication.",
      "example": "# Basic function factory\nmake_power <- function(n) {\n  force(n)  # Ensure n is evaluated\n  function(x) {\n    x^n\n  }\n}\n\n# Create specific functions\nsquare <- make_power(2)\ncube <- make_power(3)\nsqrt_reciprocal <- make_power(-0.5)\n\n# Data transformation factory\nmake_transformer <- function(center = TRUE, scale = TRUE, \n                             log_transform = FALSE, \n                             remove_outliers = FALSE,\n                             outlier_sd = 3) {\n  function(x) {\n    if (remove_outliers) {\n      mean_x <- mean(x, na.rm = TRUE)\n      sd_x <- sd(x, na.rm = TRUE)\n      x[abs(x - mean_x) > outlier_sd * sd_x] <- NA\n    }\n    \n    if (log_transform) {\n      x <- log1p(x)  # log(1 + x) to handle zeros\n    }\n    \n    if (center || scale) {\n      x <- scale(x, center = center, scale = scale)[, 1]\n    }\n    \n    x\n  }\n}\n\n# Create specific transformers\nstandardize <- make_transformer(center = TRUE, scale = TRUE)\nlog_scale <- make_transformer(center = TRUE, scale = TRUE, log_transform = TRUE)\nrobust_transform <- make_transformer(remove_outliers = TRUE, outlier_sd = 2.5)\n\n# Validation function factory\nmake_validator <- function(rules) {\n  function(data) {\n    results <- list()\n    \n    for (col in names(rules)) {\n      if (col %in% names(data)) {\n        rule <- rules[[col]]\n        \n        if (!is.null(rule$type)) {\n          results[[paste0(col, \"_type\")]] <- \n            all(class(data[[col]]) %in% rule$type)\n        }\n        \n        if (!is.null(rule$range)) {\n          results[[paste0(col, \"_range\")]] <- \n            all(data[[col]] >= rule$range[1] & \n                data[[col]] <= rule$range[2], na.rm = TRUE)\n        }\n        \n        if (!is.null(rule$required) && rule$required) {\n          results[[paste0(col, \"_complete\")]] <- \n            !any(is.na(data[[col]]))\n        }\n        \n        if (!is.null(rule$unique) && rule$unique) {\n          results[[paste0(col, \"_unique\")]] <- \n            length(unique(data[[col]])) == nrow(data)\n        }\n      }\n    }\n    \n    list(\n      valid = all(unlist(results)),\n      details = results\n    )\n  }\n}\n\n# Create validator\nuser_validator <- make_validator(list(\n  age = list(type = \"numeric\", range = c(0, 150), required = TRUE),\n  email = list(type = \"character\", required = TRUE, unique = TRUE),\n  score = list(type = \"numeric\", range = c(0, 100))\n))\n\n# Model factory\nmake_model_pipeline <- function(formula, \n                                preprocess = identity,\n                                model_fn = lm,\n                                postprocess = identity) {\n  function(train_data, test_data = NULL) {\n    # Preprocess\n    train_processed <- preprocess(train_data)\n    \n    # Fit model\n    model <- model_fn(formula, data = train_processed)\n    \n    # Predictions\n    if (!is.null(test_data)) {\n      test_processed <- preprocess(test_data)\n      predictions <- predict(model, newdata = test_processed)\n      predictions <- postprocess(predictions)\n      \n      list(\n        model = model,\n        predictions = predictions,\n        train_data = train_processed\n      )\n    } else {\n      model\n    }\n  }\n}\n\n# Create specific pipelines\nlinear_pipeline <- make_model_pipeline(\n  mpg ~ .,\n  preprocess = function(df) df %>% select(-name),\n  model_fn = lm\n)\n\nlogistic_pipeline <- make_model_pipeline(\n  am ~ hp + wt,\n  model_fn = function(f, data) glm(f, data, family = binomial()),\n  postprocess = function(p) ifelse(p > 0.5, 1, 0)\n)"
    },
    "tidyeval": {
      "description": "Use tidy evaluation for programmatic manipulation of tidyverse functions.",
      "whenToUse": "When writing functions that use dplyr/tidyverse verbs with user-supplied column names.",
      "example": "library(rlang)\n\n# Basic tidyeval function\nsummarize_by <- function(data, group_var, summary_var, .fn = mean) {\n  data %>%\n    group_by({{ group_var }}) %>%\n    summarise(\n      result = .fn({{ summary_var }}, na.rm = TRUE),\n      n = n(),\n      .groups = \"drop\"\n    )\n}\n\n# Usage\nmtcars %>% summarize_by(cyl, mpg)\nmtcars %>% summarize_by(gear, hp, .fn = median)\n\n# Multiple variables with ...\nmulti_summary <- function(data, ..., .by) {\n  data %>%\n    group_by({{ .by }}) %>%\n    summarise(\n      across(c(...), list(\n        mean = ~mean(.x, na.rm = TRUE),\n        sd = ~sd(.x, na.rm = TRUE),\n        n = ~sum(!is.na(.x))\n      )),\n      .groups = \"drop\"\n    )\n}\n\n# Usage\nmtcars %>% multi_summary(mpg, hp, wt, .by = cyl)\n\n# Dynamic column creation\ncreate_ratio <- function(data, numerator, denominator, \n                         name = \"ratio\") {\n  data %>%\n    mutate(\n      \"{name}\" := {{ numerator }} / {{ denominator }}\n    )\n}\n\n# Usage\nmtcars %>% \n  create_ratio(mpg, wt, name = \"efficiency\") %>%\n  create_ratio(hp, wt, name = \"power_weight_ratio\")\n\n# Conditional transformation\ntransform_if_exists <- function(data, column, transformation) {\n  col_name <- as_label(enquo(column))\n  \n  if (col_name %in% names(data)) {\n    data %>%\n      mutate({{ column }} := transformation({{ column }}))\n  } else {\n    warning(\"Column '\", col_name, \"' not found\")\n    data\n  }\n}\n\n# Advanced: Custom dplyr-like verb\nfilter_range <- function(data, var, min = -Inf, max = Inf) {\n  var_expr <- enquo(var)\n  \n  filter_exp <- expr(\n    !!var_expr >= !!min & !!var_expr <= !!max\n  )\n  \n  data %>%\n    filter(!!filter_exp)\n}\n\n# String-based column selection\nselect_contains <- function(data, patterns) {\n  cols <- patterns %>%\n    map(~matches(.x)) %>%\n    reduce(union)\n  \n  data %>%\n    select(!!!cols)\n}\n\n# Programmatic grouping and summarizing\nflexible_summary <- function(data, group_vars, summary_specs) {\n  # group_vars: character vector of grouping variables\n  # summary_specs: named list of summary specifications\n  \n  group_syms <- syms(group_vars)\n  \n  summary_exprs <- imap(summary_specs, function(fn, name) {\n    expr((!!name) := (!!fn))\n  })\n  \n  data %>%\n    group_by(!!!group_syms) %>%\n    summarise(!!!summary_exprs, .groups = \"drop\")\n}\n\n# Usage\nspecs <- list(\n  avg_mpg = expr(mean(mpg, na.rm = TRUE)),\n  total_hp = expr(sum(hp)),\n  cars = expr(n())\n)\n\nmtcars %>%\n  flexible_summary(c(\"cyl\", \"gear\"), specs)"
    },
    "error_handling": {
      "description": "Implement robust error handling with tryCatch, safely, and possibly.",
      "whenToUse": "For production code that needs to handle errors gracefully.",
      "example": "# Basic tryCatch\nsafe_divide <- function(x, y) {\n  tryCatch(\n    {\n      if (y == 0) {\n        stop(\"Division by zero\")\n      }\n      x / y\n    },\n    error = function(e) {\n      message(\"Error: \", e$message)\n      return(NA)\n    },\n    warning = function(w) {\n      message(\"Warning: \", w$message)\n      return(x / y)\n    },\n    finally = {\n      # Cleanup code here\n    }\n  )\n}\n\n# Using purrr's safely\nsafe_log <- safely(log)\n\nresults <- map(c(10, -5, 0, 20, \"text\"), safe_log)\n\n# Extract results and errors\nvalues <- map(results, \"result\")\nerrors <- map(results, \"error\")\n\n# Create safe version of any function\nmake_safe <- function(fn, otherwise = NA) {\n  function(...) {\n    tryCatch(\n      fn(...),\n      error = function(e) {\n        warning(\"Function failed: \", e$message)\n        otherwise\n      }\n    )\n  }\n}\n\nsafe_mean <- make_safe(mean, otherwise = 0)\n\n# Possibly - similar to safely but returns default on error\npossible_parse <- possibly(parse_number, otherwise = NA)\n\nnumbers <- c(\"123\", \"45.6\", \"abc\", \"78.9\", \"not a number\")\nparsed <- map_dbl(numbers, possible_parse)\n\n# Comprehensive error handling function\nrobust_analysis <- function(data, config) {\n  # Initialize results\n  results <- list(\n    success = FALSE,\n    data = NULL,\n    errors = list(),\n    warnings = list()\n  )\n  \n  # Validation with error collection\n  validation_errors <- list()\n  \n  if (!is.data.frame(data)) {\n    validation_errors$data_type <- \"Input must be a data frame\"\n  }\n  \n  if (nrow(data) == 0) {\n    validation_errors$empty_data <- \"Data frame is empty\"\n  }\n  \n  required_cols <- config$required_columns\n  missing_cols <- setdiff(required_cols, names(data))\n  if (length(missing_cols) > 0) {\n    validation_errors$missing_columns <- \n      paste(\"Missing columns:\", paste(missing_cols, collapse = \", \"))\n  }\n  \n  # Return early if validation fails\n  if (length(validation_errors) > 0) {\n    results$errors <- validation_errors\n    return(results)\n  }\n  \n  # Main processing with error handling\n  withCallingHandlers(\n    {\n      # Step 1: Data cleaning\n      cleaned_data <- tryCatch(\n        {\n          data %>%\n            filter(complete.cases(.)) %>%\n            mutate(across(where(is.numeric), ~replace_na(.x, median(.x, na.rm = TRUE))))\n        },\n        error = function(e) {\n          results$errors$cleaning <<- e$message\n          NULL\n        }\n      )\n      \n      if (is.null(cleaned_data)) return(results)\n      \n      # Step 2: Analysis\n      analysis_results <- tryCatch(\n        {\n          list(\n            summary = summary(cleaned_data),\n            correlations = cor(select_if(cleaned_data, is.numeric)),\n            model = lm(config$formula, data = cleaned_data)\n          )\n        },\n        error = function(e) {\n          results$errors$analysis <<- e$message\n          NULL\n        }\n      )\n      \n      if (!is.null(analysis_results)) {\n        results$success <- TRUE\n        results$data <- analysis_results\n      }\n    },\n    warning = function(w) {\n      results$warnings <<- append(results$warnings, w$message)\n      invokeRestart(\"muffleWarning\")\n    }\n  )\n  \n  results\n}\n\n# Retry logic\nwith_retry <- function(fn, max_attempts = 3, delay = 1) {\n  attempt <- 1\n  \n  while (attempt <= max_attempts) {\n    result <- tryCatch(\n      {\n        list(success = TRUE, value = fn())\n      },\n      error = function(e) {\n        list(success = FALSE, error = e$message)\n      }\n    )\n    \n    if (result$success) {\n      return(result$value)\n    }\n    \n    if (attempt < max_attempts) {\n      message(\"Attempt \", attempt, \" failed. Retrying...\")\n      Sys.sleep(delay * attempt)  # Exponential backoff\n    }\n    \n    attempt <- attempt + 1\n  }\n  \n  stop(\"All attempts failed. Last error: \", result$error)\n}\n\n# Usage\nresult <- with_retry(function() {\n  # Simulated API call that might fail\n  if (runif(1) < 0.7) stop(\"Random failure\")\n  return(\"Success!\")\n})"
    },
    "memoise": {
      "description": "Cache function results to avoid redundant computations.",
      "whenToUse": "For expensive computations that are called repeatedly with the same inputs.",
      "example": "library(memoise)\n\n# Basic memoisation\nexpensive_calculation <- function(n) {\n  message(\"Computing for n = \", n)\n  Sys.sleep(1)  # Simulate expensive operation\n  sum(1:n)\n}\n\n# Create memoised version\nfast_calculation <- memoise(expensive_calculation)\n\n# First call is slow\nsystem.time(result1 <- fast_calculation(1000000))\n\n# Subsequent calls are instant\nsystem.time(result2 <- fast_calculation(1000000))\n\n# Custom cache\ncache_dir <- \"cache/\"\ndir.create(cache_dir, showWarnings = FALSE)\n\n# Disk-based cache for persistence\npersistent_calc <- memoise(\n  expensive_calculation,\n  cache = cache_filesystem(cache_dir)\n)\n\n# Time-limited cache\ntime_sensitive_api <- memoise(\n  function(endpoint) {\n    message(\"Fetching from API: \", endpoint)\n    # Simulated API call\n    list(data = runif(10), timestamp = Sys.time())\n  },\n  cache = cache_memory(max_age = 300)  # 5 minutes\n)\n\n# Complex memoisation example\ncreate_cached_analyzer <- function() {\n  # Private cache\n  cache <- new.env(parent = emptyenv())\n  \n  analyze <- function(data, method = \"full\") {\n    # Create cache key\n    key <- digest::digest(list(data, method))\n    \n    # Check cache\n    if (exists(key, envir = cache)) {\n      message(\"Returning cached result\")\n      return(get(key, envir = cache))\n    }\n    \n    message(\"Performing analysis...\")\n    \n    # Expensive analysis\n    result <- switch(method,\n      full = {\n        list(\n          summary = summary(data),\n          model = lm(y ~ ., data = data),\n          plots = list(\n            hist = ggplot(data, aes(x = y)) + geom_histogram(),\n            scatter = ggplot(data, aes(x = x1, y = y)) + geom_point()\n          )\n        )\n      },\n      quick = {\n        list(summary = summary(data))\n      }\n    )\n    \n    # Store in cache\n    assign(key, result, envir = cache)\n    \n    result\n  }\n  \n  # Return function with methods\n  list(\n    analyze = analyze,\n    clear_cache = function() {\n      rm(list = ls(cache), envir = cache)\n      message(\"Cache cleared\")\n    },\n    cache_info = function() {\n      list(\n        size = length(ls(cache)),\n        memory = object.size(cache)\n      )\n    }\n  )\n}\n\n# Usage\nanalyzer <- create_cached_analyzer()\n\n# First run is slow\ndata <- data.frame(x1 = rnorm(1000), x2 = rnorm(1000), y = rnorm(1000))\nresult1 <- analyzer$analyze(data, \"full\")\n\n# Second run is instant\nresult2 <- analyzer$analyze(data, \"full\")\n\n# Check cache\nanalyzer$cache_info()\n\n# Selective memoisation\nmemoize_expensive_only <- function(fn) {\n  force(fn)\n  \n  function(...) {\n    args <- list(...)\n    \n    # Only memoize if computation is expected to be expensive\n    if (is.data.frame(args[[1]]) && nrow(args[[1]]) > 1000) {\n      memo_fn <- memoise(fn)\n      memo_fn(...)\n    } else {\n      fn(...)\n    }\n  }\n}\n\n# Advanced: Memoisation with side effects\nsmart_cache <- function(fn, cache_condition = function(x) TRUE) {\n  cache <- new.env()\n  \n  function(...) {\n    args <- list(...)\n    \n    if (cache_condition(args)) {\n      key <- digest::digest(args)\n      \n      if (exists(key, cache)) {\n        return(cache[[key]])\n      }\n      \n      result <- fn(...)\n      cache[[key]] <- result\n      result\n    } else {\n      fn(...)\n    }\n  }\n}"
    },
    "reactive_programming": {
      "description": "Use reactive programming patterns for dynamic data flows.",
      "whenToUse": "In Shiny applications or when building interactive data pipelines.",
      "example": "library(shiny)\nlibrary(reactiveValues)\n\n# Basic reactive pattern in Shiny\nui <- fluidPage(\n  titlePanel(\"Reactive Data Analysis\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"dataset\", \"Choose dataset:\",\n                  choices = c(\"mtcars\", \"iris\", \"airquality\")),\n      \n      uiOutput(\"variable_selector\"),\n      \n      sliderInput(\"bins\", \"Number of bins:\", \n                  min = 5, max = 50, value = 30),\n      \n      actionButton(\"analyze\", \"Run Analysis\")\n    ),\n    \n    mainPanel(\n      tabsetPanel(\n        tabPanel(\"Plot\", plotOutput(\"histogram\")),\n        tabPanel(\"Summary\", verbatimTextOutput(\"summary\")),\n        tabPanel(\"Model\", verbatimTextOutput(\"model\"))\n      )\n    )\n  )\n)\n\nserver <- function(input, output, session) {\n  # Reactive values\n  values <- reactiveValues(\n    data = NULL,\n    analysis_count = 0\n  )\n  \n  # Reactive expression with caching\n  getData <- reactive({\n    req(input$dataset)\n    \n    data <- switch(input$dataset,\n      \"mtcars\" = mtcars,\n      \"iris\" = iris,\n      \"airquality\" = airquality\n    )\n    \n    # Store in reactive values\n    values$data <- data\n    data\n  })\n  \n  # Reactive conductor\n  getNumericVars <- reactive({\n    data <- getData()\n    names(data)[sapply(data, is.numeric)]\n  })\n  \n  # Dynamic UI\n  output$variable_selector <- renderUI({\n    choices <- getNumericVars()\n    selectInput(\"variable\", \"Select variable:\",\n                choices = choices,\n                selected = choices[1])\n  })\n  \n  # Event reactive - only updates on button click\n  analysis <- eventReactive(input$analyze, {\n    req(input$variable)\n    \n    data <- getData()\n    var_data <- data[[input$variable]]\n    \n    values$analysis_count <- values$analysis_count + 1\n    \n    list(\n      variable = input$variable,\n      data = var_data,\n      summary = summary(var_data),\n      model = if(input$variable != \"Species\" && \"mpg\" %in% names(data)) {\n        lm(as.formula(paste(input$variable, \"~ .\")), data = data)\n      } else {\n        NULL\n      }\n    )\n  })\n  \n  # Debounced reactive\n  debouncedBins <- debounce(reactive(input$bins), 500)\n  \n  # Outputs\n  output$histogram <- renderPlot({\n    req(analysis())\n    \n    hist(analysis()$data, \n         breaks = debouncedBins(),\n         main = paste(\"Histogram of\", analysis()$variable),\n         xlab = analysis()$variable,\n         col = \"lightblue\",\n         border = \"white\")\n  })\n  \n  output$summary <- renderPrint({\n    req(analysis())\n    \n    cat(\"Analysis #\", values$analysis_count, \"\\n\\n\")\n    print(analysis()$summary)\n  })\n  \n  output$model <- renderPrint({\n    req(analysis()$model)\n    summary(analysis()$model)\n  })\n  \n  # Observe for side effects\n  observe({\n    cat(\"Dataset changed to:\", input$dataset, \"\\n\")\n  })\n  \n  # ObserveEvent for specific triggers\n  observeEvent(input$analyze, {\n    showNotification(\n      paste(\"Analysis\", values$analysis_count, \"completed\"),\n      type = \"success\",\n      duration = 3\n    )\n  })\n}\n\n# Non-Shiny reactive pattern\ncreate_reactive_pipeline <- function() {\n  # Internal state\n  .values <- new.env()\n  .observers <- list()\n  \n  # Reactive value setter\n  set_reactive <- function(name, value) {\n    old_value <- .values[[name]]\n    .values[[name]] <- value\n    \n    # Trigger observers\n    if (name %in% names(.observers)) {\n      for (observer in .observers[[name]]) {\n        observer(value, old_value)\n      }\n    }\n  }\n  \n  # Reactive value getter\n  get_reactive <- function(name) {\n    .values[[name]]\n  }\n  \n  # Add observer\n  observe <- function(name, callback) {\n    if (!(name %in% names(.observers))) {\n      .observers[[name]] <<- list()\n    }\n    .observers[[name]] <<- append(.observers[[name]], callback)\n  }\n  \n  # Computed reactive\n  computed <- function(name, dependencies, compute_fn) {\n    update_computed <- function(...) {\n      values <- lapply(dependencies, get_reactive)\n      result <- do.call(compute_fn, values)\n      set_reactive(name, result)\n    }\n    \n    # Set up observers for dependencies\n    for (dep in dependencies) {\n      observe(dep, function(...) update_computed())\n    }\n    \n    # Initial computation\n    update_computed()\n  }\n  \n  list(\n    set = set_reactive,\n    get = get_reactive,\n    observe = observe,\n    computed = computed\n  )\n}\n\n# Usage example\npipeline <- create_reactive_pipeline()\n\n# Set reactive values\npipeline$set(\"x\", 10)\npipeline$set(\"y\", 20)\n\n# Computed reactive\npipeline$computed(\"sum\", c(\"x\", \"y\"), function(x, y) x + y)\npipeline$computed(\"product\", c(\"x\", \"y\"), function(x, y) x * y)\n\n# Add observers\npipeline$observe(\"sum\", function(new, old) {\n  cat(\"Sum changed from\", old, \"to\", new, \"\\n\")\n})\n\n# Trigger updates\npipeline$set(\"x\", 15)  # This will update sum and product"
    }
  }
}